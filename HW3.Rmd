---
title: "Time Series HW 3"
author: |
  | Kenny Flagg and Paul Harmon
  | __who have not yet worked together and want the bonus!__
date: "September 16, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW 3

_You can alone or in pairs that may or may not be people you worked with before. You can discuss it with old partners but should try work as much as possible with new collaborators. 5% bonus if you find someone completely new to work with - that you did not work with on first two assignments._

1) _For the Bozeman temperature data from HW 1 and 2, estimate a model with month only, subtract its fitted values from the responses (or just extract residuals). Plot the residuals vs the fractional `Year` variable and compare the plot of this result to the plot of the original time series._
The plot of the residuals and the plot of the original time series are shown below. Note that because the residuals have mean 0, the plot fo the residuals is centered around 0 whereas the plot of the original TS is still on the temperature scale. However, the same increasing linear trend appears evident in both plots. 

    ```{r problem1}
weather = read.csv("rawbozemandata.csv", header = TRUE)

weather$year = (floor(weather$DATE/100))
weather$Month<-round((weather$DATE/100-weather$year)*100,1)
weather$Yearfrac<-weather$year+(weather$Month-1)/12 #makes it an integer variable
#fit the linear model
lm.weather = lm(MMXT ~ Month, data = weather)
#lm.weather$residuals
par(mfrow = c(1,2))
plot(weather$Yearfrac, lm.weather$residuals, pch = 20, col = "orange4",
     main = "Residuals vs. Year", xlab = "Year", ylab = "Residuals")
plot(weather$Yearfrac, weather$MMXT, pch = 20, col = "blue2",main = "Original TS", xlab = "Year", ylab = "MMXT")

```


2) _In the de-seasonalized Bozeman temperature data set, re-assess evidence for the linear trend. Compare the result (test statistic, degrees of freedom and size of p-value) of just fitting a linear time trend in these de-seasonalized responses to the result from our original model with a linear year component and a month adjustment (not the quadratic trend model)._

    In the tables below, we list the tests for the linear year-based trend.  The test for the original model is a $t_{1371}$ as is the test for the deseasonalized model.  In both models we find strong evidence in favor of the linear time trend; the seasonalized model yields a p-value of 0.00014 and the deseasonalized model yields a p-value of 0.00014. Further, the raw slopes are very similar - this indicates that the deseasonalization of the data preserved the long-term time trend. 

```{r problem2}
library(pander) #because Paul thinks "library" is cooler than "require"
panderOptions('missing', '-') 
lm.seasoned <- lm(MMXT ~ Yearfrac + Month, data = weather)
lm.deseasoned <- lm(lm.weather$residuals ~ Yearfrac + Month, data = weather)
pander(summary(lm.seasoned))
pander(summary(lm.deseasoned))
```

     
3) _I briefly discussed the HADCRUT data set in class. We will consider the HADCRUT4 series of temperature anomalies for the Nothern Hemisphere. The fully up to date data set is available at: `http://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/time_series/HadCRUT.4.4.0.0.monthly_nh.txt`_

    _Download the ensemble median monthly northern hemisphere temperature data. We will use the entire time series that is currently available (January 1850 to July 2016). You might want to read `http://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/series_format.html` for more information on the columns in the data set._

     _Because the time series is complete over the time frame under consideration, you can use `ts()` to create a time variable instead of messing around with their `Year/Month` variable._

    _Make a plot versus time of the ensemble medians and use that as your response variable in the following questions. Discuss trend, seasonality, outliers, and variability._

    The plot of median temperature anomaly over time appears to show some increase in temperatures over time. Further, the variability of temperatures was wider during early times, likely due to the fact that people measuring temperatures in the late 1800s/early 1900s were using less precise methods to measure temperature. There are a couple of outliers in the negative direction (there was an anomaly of -1.509 in 1893) but not any temperatures that appear to be outlying in the positive direction, especially given the increasing trend. We definitely see seasonal oscilations in the data; this is to be expected. 

```{r problem3}
par(mfrow = c(1,1))
    hadcrut <- read.table('HadCRUT.4.4.0.0.monthly_nh.txt',
                      col.names = c('Date', 'median.temp.anom',
                                    'bias.l95', 'bias.u95',
                                    'meas.l95', 'meas.u95',
                                    'coverage.l95', 'coverage.u95',
                                    'error.l95', 'error.u95',
                                    'l95', 'u95'))
hadcrut.ts <- ts(hadcrut$median.temp.anom, start = 1850, frequency = 12)
plot(hadcrut.ts, ylab = "Median Temperature Anomaly", main = "Temperature Anomaly over Time", col = "forestgreen")
```

4) _Our main focus with these data will be on estimating the long-term trend, starting with polynomial trend models. But first, check for seasonality in a model that accounts for a linear time trend. Use our previous fractional year for the trend. Report an `effects` plot and a test for the month model component._

    _Note: when you use `time()` to generate the `Year` variable from a time series object it retains some time series object information that can cause conflicts later on. Create a new variable in your data.frame that uses something like `as.vector(time(tsdataname))`._

  Based on the F test (using Type III Sum of Squares), $F_{11, 1986}$, the p-value is very small. This indicates strong evidence in favor of a month effect after accounting for everything else in the model (i.e. the linear year trend). 

    ```{r problem4, message = FALSE}
hadcrut$yearfrac <- as.vector(time(hadcrut.ts))
hadcrut$month <- factor(round(1 + 12 * (hadcrut$yearfrac %% 1), 0))
had.lm1 <- lm(median.temp.anom ~ month + yearfrac, data = hadcrut)
summary(had.lm1)
library(car)
pander(Anova(had.lm1, test = "F", type = "III")) #test for the month component

library(effects)
plot(allEffects(had.lm1))
```

5) _Check the residuals versus fitted values for any evidence of nonlinearity in the residuals vs fitted that was missed by the model with a linear trend and month component. Also note any potential issues with the constant variance assumption._

    Based on the plot below, there appears to be a wavy pattern occuring in the residuals over time that is not being accounted for in the model with a linear trend and month component.  Furthermore, the spread of observations is not constant; it appears that the spread for smaller fitted values is wider near fitted values of -0.5 and much narrower for larger fitted values. 
    
    ```{r problem5}
plot(had.lm1$fitted.values, had.lm1$residuals, pch = 20, col = "blue2", main = "Residuals vs. Fitted Values", xlab = "Fitted Values", ylab = "Residuals")
```

6) _You can add higher order polynomial terms to models using `x1+I(x1^2)+I(x1^3)`... or using the `poly` function, such as `poly(x1,3,raw=T)` for a cubic polynomial that includes the linear and quadratic components (we want this!). The `raw=T` keeps the variables in their raw or input format. Estimate the same model but now using polynomial trends that steps up from linear (poly(time,1,raw=T)) and stop when you get a failure to estimate a part of the model. Briefly discuss what happened._

    The following code estimates the ith-degree polynomial, where i ranges from 1 to 10. Thus, we see that from the returned summary output we get estimates for the coefficients up to the 5th degree polynomial. At the fifth-degree polynomial estimate, the model failed to estimate a coefficient; it also failed on several other higher-ordered terms. Looking at the residuals vs. fitted values plot previously produced, it appears that a fourth-degree polynomial would be appropriate but the graph indicates no reason that the model could not be fit to a higher-ordered model. Theoretically, we could fit a very high (1982)-degree polynomial to these data. 

    ```{r problem6}
for (i in 1:10){
  had.lm.poly = lm(median.temp.anom ~ poly(yearfrac,i,raw = TRUE) + month, data = hadcrut)
  print(summary(had.lm.poly))
}
```


7) _If we center or, even better, make the polynomial functions orthogonal to one another, we can avoid the issue in the previous question. Use `poly(x1,?,raw=F)` and step up the polynomial order for time until the p-value for the last coefficient (use `summary()`) is "large", reporting the single test result for each step in the building process. Then drop back one order and re-fit the model. Report the `effects` plot of the resulting model and describe the estimated trend. Note: aside from access to orthogonal polynomials the `poly` function interfaces with `Anova` and the `effects` package really nicely._

    The first coefficient with a "large" p-value was the 11th-order coefficient. Thus, we re-fit the model with a 10th order polynomial. The effects plot of the resulting model is also included. It appears that there was a decreasing trend until about 1900 followed by a relatively sharp increase after the turn of the century. It appears that the increase was steepest during the 1950's. 

    ```{r problem7}
pvals <- numeric(20)
for (i in 1:20){
  lm.poly.1 = lm(median.temp.anom ~ poly(yearfrac,i,raw = FALSE)+ month, data = hadcrut) 
  pvals[i] = (summary(lm.poly.1)$coefficients[i,4])
}
pvals
which(pvals > 0.05)
lm.poly.final = lm(median.temp.anom ~ poly(yearfrac,10,raw = FALSE) + month , data = hadcrut)
plot(allEffects(lm.poly.final))
```

8) _Check the diagnostic plots from your final model. Does anything improve from the first version. Also plot the residuals vs time and compare that plot to residuals vs fitted._

    Based on the plots below, we can see that nothing appeared to improve compared to the first version. The issues of non-constant variance appear to be as bad or worse than before; the plot of residuals vs. fitted values is funnel shaped rather than relatively uniform. The issues with non-linearity may be improved somewhat; however, this may be an artifact of the scaling of the plot. From the QQ plot, the tails may be a little heavy but the assumption of normality is not likely violated. 
    Further, the plot of residuals vs. time does appear somewhat less curvy than before. Given that imposing higher-ordered polynomial terms into the model is unlikely to solve any problems with heteroskedasticity, and that the goal of using them was to account for non-linear trends in the data, it appears that the inclusion of higher-ordered terms was at least somewhat effective in mitigating the curviness of the trend. 

    ```{r problem8}
par(mfrow = c(2,2))
plot(lm.poly.final)
par(mfrow = c(1,1))
plot(hadcrut$yearfrac,resid(lm.poly.final), main = "Residuals vs.Time",xlab = "Time", ylab = "Residuals", pch = 20)
```


9) _Run the following code so I can see what version of R you are now using:_

### Documenting R version 

```{r}
print(sessionInfo()$R.version$nickname)
getRversion()
print(sessionInfo()$platform)
```
